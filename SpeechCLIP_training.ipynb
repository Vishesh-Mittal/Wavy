{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3S4aZCdPjLC",
        "outputId": "b873b5c7-4463-4ffc-de9e-723a7329f2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk5RJzMjleg-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b-7Sh53Hi8C",
        "outputId": "361e6f3a-3a2f-49eb-9116-62b8b646c857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ6s5tN0lv42"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/SpeechDiffusion/speechcoco-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARXAbcH3a4_h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wav2cap = pd.read_csv('data/wav2capt.txt', sep = ' ', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx8bf7xvbgxQ"
      },
      "outputs": [],
      "source": [
        "wav2cap_dict = dict(zip(wav2cap[0], wav2cap[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PDHqz6Youwq"
      },
      "outputs": [],
      "source": [
        "# x, _ = sf.read(f'data/wavs/{wav2cap[0].values[0]}', samplerate=None)\n",
        "# print(len(processor(x).input_values[0]))\n",
        "\n",
        "# x, _ = sf.read(f'data/wavs/{wav2cap[0].values[1]}', samplerate=None)\n",
        "# print(len(processor(x).input_values[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwPhj2F7rIhQ"
      },
      "outputs": [],
      "source": [
        "# raw_lengths = []\n",
        "# lengths = []\n",
        "# processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "# print(len(wav2cap[0].values))\n",
        "\n",
        "# for i in wav2cap[0].values:\n",
        "#   print(i)\n",
        "#   raw, _ = sf.read(f\"data/wavs/{i}\")\n",
        "#   raw_lengths.append(len(raw))\n",
        "\n",
        "#   processed = processor(raw, return_tensors=\"pt\").input_values[0]\n",
        "#   lengths.append(len(processed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnoRetldfYvO"
      },
      "outputs": [],
      "source": [
        "# samples_500 = list(set(wav2cap[1]))[:500]\n",
        "\n",
        "with open('/content/drive/MyDrive/SpeechDiffusion/samples500.txt', 'r') as file:\n",
        "  samples_500 = [line.strip() for line in file.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEENbLjp5yi7"
      },
      "outputs": [],
      "source": [
        "subset500 = wav2cap[wav2cap[1].isin(samples_500)]\n",
        "wav2cap_subset500_dict = dict(zip(subset500[0], subset500[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNRGqz9i5gOr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "\n",
        "from torchvision import transforms\n",
        "from transformers import AutoProcessor, HubertModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Get the number of available processors\n",
        "num_processors = os.cpu_count()\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "def collate_batch(batch):\n",
        "    images, audio_sequences = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Pad the audio sequences to have the same length\n",
        "    audio_sequences = pad_sequence(audio_sequences, batch_first=True)\n",
        "\n",
        "    return images, audio_sequences\n",
        "\n",
        "# Example dataset class\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, wav2cap_dict, speech_processor, image_size=224, data_path='data'):\n",
        "        self.caption_filenames = list(wav2cap_dict.keys())\n",
        "        self.transform = transforms.Compose([\n",
        "                    transforms.Resize((image_size, image_size)),\n",
        "                    transforms.ToTensor()])\n",
        "\n",
        "        self.wav2cap_dict = wav2cap_dict\n",
        "        self.data_path = data_path\n",
        "        self.speech_processor = speech_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        item = self.caption_filenames[index]\n",
        "\n",
        "        # IMAGE\n",
        "        image = Image.open(os.path.join(self.data_path, 'images', self.wav2cap_dict.get(item)))\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # SPEECH\n",
        "        raw_speech, sampling_rate = sf.read(os.path.join(self.data_path, 'wavs', item))\n",
        "        speech_output = self.speech_processor(raw_speech, return_tensors=\"pt\", sampling_rate=sampling_rate).input_values.squeeze(0)\n",
        "        # print(speech_output[0].shape)\n",
        "        # return image, raw_speech\n",
        "\n",
        "        return image, speech_output\n",
        "\n",
        "# dataset = ImageTextDataset(wav2cap_dict, processor)\n",
        "dataset = ImageTextDataset(wav2cap_dict, processor)\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=collate_batch,\n",
        "                        num_workers=num_processors,\n",
        "                        pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgvYNkrX7pqi"
      },
      "source": [
        "### **Defining Speech Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bEeQLfx-DTZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SpeechProjection(nn.Module):\n",
        "    def __init__(self, speech_embedding_size, shared_embedding_size, dropout=0.1):\n",
        "        super(SpeechProjection, self).__init__()\n",
        "        self.speech_projection = nn.Linear(speech_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        projected_embeddings = self.speech_projection(text_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x # projected_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqkHdZhR7s61"
      },
      "source": [
        "### **Defining Image Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9TcERlI7vDB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, ViTModel\n",
        "\n",
        "# image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# inputs = image_processor(image, return_tensors=\"pt\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(**inputs)\n",
        "\n",
        "# last_hidden_states = outputs.last_hidden_state\n",
        "# list(last_hidden_states.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6u74A8IUwc6"
      },
      "outputs": [],
      "source": [
        "class ImageProjection(nn.Module):\n",
        "    def __init__(self, image_embedding_size, shared_embedding_size, dropout=0.1):\n",
        "        super(ImageProjection, self).__init__()\n",
        "        self.image_projection = nn.Linear(image_embedding_size, shared_embedding_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(shared_embedding_size, shared_embedding_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(shared_embedding_size)\n",
        "\n",
        "    def forward(self, image_embeddings):\n",
        "        projected_embeddings = self.image_projection(image_embeddings)\n",
        "\n",
        "        x = self.gelu(projected_embeddings)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected_embeddings\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x # projected_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZA454TYMpk"
      },
      "source": [
        "### **CLIP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egWCKgmqYOaF"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "\n",
        "temperature_value = 1\n",
        "\n",
        "def contrastive_clip_loss_function(text_projection,  image_projection, mode=\"eval\"):\n",
        "    logits = (text_projection @ image_projection.T) / temperature_value\n",
        "    if mode==\"train\":\n",
        "        images_similarity = image_projection @ image_projection.T\n",
        "        texts_similarity = text_projection @ text_projection.T\n",
        "        targets = F.softmax( (images_similarity + texts_similarity) / 2 * temperature_value, dim=-1 )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "        return loss.mean()\n",
        "    elif mode==\"eval\":\n",
        "        return logits\n",
        "    else:\n",
        "        print(\"Mention mode\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvtd2GtmiUvZ"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AseXHfWVYTZW"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wI6cMm975Vk",
        "outputId": "29ed8be2-4e1a-488f-ee8d-d250e01b2558"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0/313, Loss:8.154058456420898\n",
            "Batch 100/313, Loss:1.7144919129291383\n",
            "Batch 200/313, Loss:1.0697100924615244\n",
            "Batch 300/313, Loss:0.8163890529708608\n",
            "Epoch [1/15], Average Loss: 0.7957\n",
            "Epoch: 2\n",
            "Batch 0/313, Loss:0.3013402223587036\n",
            "Batch 100/313, Loss:0.2774483220707072\n",
            "Batch 200/313, Loss:0.27793322221853245\n",
            "Batch 300/313, Loss:0.27582898330054806\n",
            "Epoch [2/15], Average Loss: 0.2757\n",
            "Epoch: 3\n",
            "Batch 0/313, Loss:0.28349706530570984\n",
            "Batch 100/313, Loss:0.2710456057350234\n",
            "Batch 200/313, Loss:0.2716118312297176\n",
            "Batch 300/313, Loss:0.270365630075385\n",
            "Epoch [3/15], Average Loss: 0.2698\n",
            "Epoch: 4\n",
            "Batch 0/313, Loss:0.2640957832336426\n",
            "Batch 100/313, Loss:0.2659936424824271\n",
            "Batch 200/313, Loss:0.2658328067752259\n",
            "Batch 300/313, Loss:0.2656818027314158\n",
            "Epoch [4/15], Average Loss: 0.2653\n",
            "Epoch: 5\n",
            "Batch 0/313, Loss:0.25839895009994507\n",
            "Batch 100/313, Loss:0.2646292191330749\n",
            "Batch 200/313, Loss:0.2640813586130664\n",
            "Batch 300/313, Loss:0.2636996157640635\n",
            "Epoch [5/15], Average Loss: 0.2634\n",
            "Epoch: 6\n",
            "Batch 0/313, Loss:0.2619117200374603\n",
            "Batch 100/313, Loss:0.2636436094151865\n",
            "Batch 200/313, Loss:0.26325241554139267\n",
            "Batch 300/313, Loss:0.2631417962898844\n",
            "Epoch [6/15], Average Loss: 0.2628\n",
            "Epoch: 7\n",
            "Batch 0/313, Loss:0.26436272263526917\n",
            "Batch 100/313, Loss:0.26188076899783447\n",
            "Batch 200/313, Loss:0.26205795009930927\n",
            "Batch 300/313, Loss:0.26193240671062784\n",
            "Epoch [7/15], Average Loss: 0.2616\n",
            "Epoch: 8\n",
            "Batch 0/313, Loss:0.26468604803085327\n",
            "Batch 100/313, Loss:0.2613199604029703\n",
            "Batch 200/313, Loss:0.2612311933467637\n",
            "Batch 300/313, Loss:0.2609919552193132\n",
            "Epoch [8/15], Average Loss: 0.2607\n",
            "Epoch: 9\n",
            "Batch 0/313, Loss:0.2611129879951477\n",
            "Batch 100/313, Loss:0.25798238120456735\n",
            "Batch 200/313, Loss:0.21047294807078235\n",
            "Batch 300/313, Loss:0.1856861320444912\n",
            "Epoch [9/15], Average Loss: 0.1838\n",
            "Epoch: 10\n",
            "Batch 0/313, Loss:0.13093449175357819\n",
            "Batch 100/313, Loss:0.13895231824700194\n",
            "Batch 200/313, Loss:0.13601554025761523\n",
            "Batch 300/313, Loss:0.13860321129081257\n",
            "Epoch [10/15], Average Loss: 0.1385\n",
            "Epoch: 11\n",
            "Batch 0/313, Loss:0.13532255589962006\n",
            "Batch 100/313, Loss:0.13624315922803218\n",
            "Batch 200/313, Loss:0.13464888433615366\n",
            "Batch 300/313, Loss:0.1346236192110765\n",
            "Epoch [11/15], Average Loss: 0.1344\n",
            "Epoch: 12\n",
            "Batch 0/313, Loss:0.13309527933597565\n",
            "Batch 100/313, Loss:0.13180721750353822\n",
            "Batch 200/313, Loss:0.13243478581086912\n",
            "Batch 300/313, Loss:0.13226348047834693\n",
            "Epoch [12/15], Average Loss: 0.1322\n",
            "Epoch: 13\n",
            "Batch 0/313, Loss:0.13064326345920563\n",
            "Batch 100/313, Loss:0.13295622169971466\n",
            "Batch 200/313, Loss:0.13267345941481898\n",
            "Batch 300/313, Loss:0.13291219961009548\n",
            "Epoch [13/15], Average Loss: 0.1327\n",
            "Epoch: 14\n",
            "Batch 0/313, Loss:0.13004475831985474\n",
            "Batch 100/313, Loss:0.13298985981705166\n",
            "Batch 200/313, Loss:0.13276634443162091\n",
            "Batch 300/313, Loss:0.13717014434131672\n",
            "Epoch [14/15], Average Loss: 0.1391\n",
            "Epoch: 15\n",
            "Batch 0/313, Loss:0.13399727642536163\n",
            "Batch 100/313, Loss:0.1376278720574804\n",
            "Batch 200/313, Loss:0.13565764743000713\n",
            "Batch 300/313, Loss:0.13567421498686769\n",
            "Epoch [15/15], Average Loss: 0.1356\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import itertools\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from transformers import Wav2Vec2Model\n",
        "from torch.optim import AdamW\n",
        "\n",
        "PATH = '/content/drive/MyDrive/checkpoints3.pt'\n",
        "\n",
        "shared_embedding_size = 512\n",
        "num_epochs = 15\n",
        "\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# IMAGE\n",
        "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device)\n",
        "vit_output_shape = 768\n",
        "\n",
        "image_projection = ImageProjection(image_embedding_size = vit_output_shape,\n",
        "                                   shared_embedding_size = shared_embedding_size).to(device)\n",
        "\n",
        "# SPEECH\n",
        "\n",
        "# hubert = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\").to(device)\n",
        "# hubert_output_shape = 768\n",
        "\n",
        "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
        "wav2vec2_output_shape = 768\n",
        "\n",
        "# for param in hubert.feature_extractor.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# for param in hubert.feature_projection.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "speech_projection = SpeechProjection(speech_embedding_size = wav2vec2_output_shape,\n",
        "                                     shared_embedding_size = shared_embedding_size).to(device)\n",
        "\n",
        "# Parameters\n",
        "\n",
        "# params = [{'params': vit.parameters(), 'lr':1e-4},\n",
        "#           {'params': itertools.chain(image_projection.parameters(),\n",
        "#                                      speech_projection.parameters()), 'lr':1e-3, 'weight_decay':1e-3},\n",
        "#           {'params': hubert.parameters(), 'lr':1e-3}]\n",
        "\n",
        "params = [{'params': itertools.chain(image_projection.parameters(),\n",
        "                                     speech_projection.parameters()), 'lr':3e-3, 'weight_decay':1e-3},\n",
        "          {'params': wav2vec2.parameters(), 'lr':3e-3}]\n",
        "\n",
        "# optimizer = bnb.optim.AdamW8bit(params)\n",
        "optimizer = AdamW(params)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=1 , factor=0.8, )\n",
        "\n",
        "# - - - - - - - - -  Training loop  - - - - - - - - -\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch: {epoch+1}\")\n",
        "  vit.train()\n",
        "  wav2vec2.train()\n",
        "  image_projection.train()\n",
        "  speech_projection.train()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  for batch_idx, (images, speech) in enumerate(dataloader):\n",
        "    # - - - - - - - - -  Forward pass  - - - - - - - - -\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    speech = speech.to(device)\n",
        "    speech_outputs = wav2vec2(speech)\n",
        "    speech_emb = speech_projection(speech_outputs.last_hidden_state.mean(dim=1))\n",
        "\n",
        "    images = images.to(device)\n",
        "    image_outputs = vit(images)\n",
        "    image_emb = image_projection(image_outputs.last_hidden_state.mean(dim=1))\n",
        "\n",
        "    loss = contrastive_clip_loss_function(speech_emb, image_emb, mode=\"train\")\n",
        "\n",
        "    loss.backward()  # Scale the loss to prevent underflow\n",
        "    optimizer.step()\n",
        "\n",
        "    # - - - -  Text  - - - -\n",
        "    speech = speech.to(device)\n",
        "    speech = wav2vec2(speech)\n",
        "    speech = speech_projection(speech.last_hidden_state.mean(dim=1))\n",
        "\n",
        "    # - - - -  Image  - - - -\n",
        "    images = images.to(device)\n",
        "    images = vit(images)\n",
        "    images = image_projection(images.last_hidden_state.mean(dim=1))\n",
        "\n",
        "    # - - - -  Compute Loss  - - - -\n",
        "    loss = contrastive_clip_loss_function(speech, images, mode=\"train\")\n",
        "\n",
        "    # - - - -  Backpropagation  - - - -\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # scaler.scale(loss).backward()\n",
        "    # scaler.step(optimizer)\n",
        "    # scaler.update()\n",
        "\n",
        "    # - - - - Loss print - - - -\n",
        "    total_loss += loss.item()\n",
        "    if batch_idx%100==0:\n",
        "      print(f\"Batch {batch_idx}/{len(dataloader)}, Loss:{total_loss/((batch_idx+1)*batch_size)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  avg_loss = total_loss / (len(dataloader) * batch_size)\n",
        "  lr_scheduler.step(avg_loss)\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "  torch.save({\n",
        "        'epoch': epoch,\n",
        "        'hubert_state_dict': wav2vec2.state_dict(),\n",
        "        'image_projection_state_dict': image_projection.state_dict(),\n",
        "        'speech_projection_state_dict': speech_projection.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss,\n",
        "        }, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbdKKdqj8tuk"
      },
      "outputs": [],
      "source": [
        "torch.save(wav2vec2.state_dict(), '/content/drive/MyDrive/speechmodel-wav2vec2.pt')\n",
        "torch.save(image_projection.state_dict(), '/content/drive/MyDrive/image_projection-w2v.pt')\n",
        "torch.save(speech_projection.state_dict(), '/content/drive/MyDrive/speech_projection3-w2v.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "JA58dGdBxsnb",
        "outputId": "241fd961-250f-4071-f056-b7c2e13117b3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d41053f47f63>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimage_projection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_image_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "vit.eval()\n",
        "image_projection.eval()\n",
        "\n",
        "def create_image_embeddings(images):\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = vit(images)\n",
        "        image_projection = image_projection(image_embeddings)\n",
        "    return image_projection\n",
        "\n",
        "image_embeddings_list_train = []\n",
        "\n",
        "for index in range(len(dataset)):\n",
        "    images = train_dataset[index][0]\n",
        "    images = images.to(device)\n",
        "    image_projection = create_image_embeddings(images.unsqueeze(0))\n",
        "    image_embeddings_list_train.append( image_projection[0] )\n",
        "\n",
        "\n",
        "def image_retrieval_function(input_query, n , display=False): # n --> number of images\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_query, return_tensors='pt', padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = text_model(**inputs)\n",
        "        text_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        text_projection = text_projector(text_embeddings)\n",
        "\n",
        "    similarity_scores_list = []\n",
        "    for index in tqdm(range(len(image_embeddings_list_train))):\n",
        "        score = torch.dot( text_projection[0], image_embeddings_list_train[index] )\n",
        "        similarity_scores_list.append( score.cpu().numpy() )\n",
        "\n",
        "    max_indexes = np.array(similarity_scores_list).argsort()[-n:][::-1]\n",
        "    if display:\n",
        "        for index in max_indexes:\n",
        "            image_tensor = train_dataset[index][0]\n",
        "            plt.imshow( torch.moveaxis(image_tensor, 0,2) )\n",
        "            plt.show()\n",
        "        return None\n",
        "    else:\n",
        "        return max_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sA-VRM3zPWO"
      },
      "outputs": [],
      "source": [
        "vit.eval()\n",
        "image_projection.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  images_embeddings = torch.Tensor()\n",
        "  for i, (image, speech) in enumerate(dataloader):\n",
        "    outputs = vit(image.to(device))\n",
        "    emb = image_projection(outputs.last_hidden_state.mean(dim=1)).cpu()\n",
        "    torch.concat((images_embeddings, emb))\n",
        "    print(f\"{i}/{len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_6GNxT6zRH9",
        "outputId": "31f6a783-f76a-4bc9-c13e-ef0d5ee842b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0999, -0.0604, -0.1155,  ...,  0.5536, -1.2522, -0.3032],\n",
              "        [-0.1201, -0.0741, -0.1117,  ...,  0.5133, -1.2332, -0.2707],\n",
              "        [-0.0889, -0.0658, -0.0913,  ...,  0.5349, -1.2376, -0.2617],\n",
              "        [-0.1038, -0.1076, -0.1018,  ...,  0.5128, -1.2385, -0.2707]])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.concat((torch.Tensor(), emb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Xv0YRFKq0W1H",
        "outputId": "df838869-cde0-4a25-fc07-715b4d6999d0"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "new(): data must be a sequence (got NoneType)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b7f4abc80e94>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got NoneType)"
          ]
        }
      ],
      "source": [
        "torch.Tensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnmvjS1E2mgr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWxegR0R3YwZ"
      },
      "outputs": [],
      "source": [
        "torch.save(hubert.state_dict(), 'hubert_3epochs.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGhIh-ox3oD3"
      },
      "outputs": [],
      "source": [
        "!cp /content/hubert_3epochs.pt /content/drive/MyDrive/hubert_3epochs.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "L0sJ3kmr32cY",
        "outputId": "a7f0c71c-c8a0-4ebd-e78e-0c497c2b147f"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 1.06 MiB is free. Process 416813 has 14.74 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 194.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-9b3bc1bc0d36>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/hubert_3epochs.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             untyped_storage = torch.UntypedStorage(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 1.06 MiB is free. Process 416813 has 14.74 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 194.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "torch.load('/content/hubert_3epochs.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "Ml6YozJr4P13",
        "outputId": "ba0714f0-8e1e-4ec7-8b79-5b9c671a89e2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-40c4c95f6e60>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhubert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/hubert_3epochs.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \"\"\"\n\u001b[1;32m   2103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected state_dict to be dict-like, got {type(state_dict)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."
          ]
        }
      ],
      "source": [
        "hubert.load_state_dict('/content/hubert_3epochs.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv3xdlao4UGd",
        "outputId": "61b742a9-6fc7-4b59-c29c-f0d5b1fa104f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kill: (416813): No such process\n"
          ]
        }
      ],
      "source": [
        "!sudo kill -9 416813"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nxZEDkr49Gn",
        "outputId": "e4b853f9-e5c2-49ff-965d-cc8724c0450f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('active.all.allocated', 13536346), ('active.all.current', 1498), ('active.all.freed', 13534848), ('active.all.peak', 1901), ('active.large_pool.allocated', 8861357), ('active.large_pool.current', 503), ('active.large_pool.freed', 8860854), ('active.large_pool.peak', 709), ('active.small_pool.allocated', 4674989), ('active.small_pool.current', 995), ('active.small_pool.freed', 4673994), ('active.small_pool.peak', 1223), ('active_bytes.all.allocated', 57968531498496), ('active_bytes.all.current', 15175254016), ('active_bytes.all.freed', 57953356244480), ('active_bytes.all.peak', 15463658496), ('active_bytes.large_pool.allocated', 56703870142976), ('active_bytes.large_pool.current', 15108582912), ('active_bytes.large_pool.freed', 56688761560064), ('active_bytes.large_pool.peak', 15396592640), ('active_bytes.small_pool.allocated', 1264661355520), ('active_bytes.small_pool.current', 66671104), ('active_bytes.small_pool.freed', 1264594684416), ('active_bytes.small_pool.peak', 98147328), ('allocated_bytes.all.allocated', 57968531498496), ('allocated_bytes.all.current', 15175254016), ('allocated_bytes.all.freed', 57953356244480), ('allocated_bytes.all.peak', 15463658496), ('allocated_bytes.large_pool.allocated', 56703870142976), ('allocated_bytes.large_pool.current', 15108582912), ('allocated_bytes.large_pool.freed', 56688761560064), ('allocated_bytes.large_pool.peak', 15396592640), ('allocated_bytes.small_pool.allocated', 1264661355520), ('allocated_bytes.small_pool.current', 66671104), ('allocated_bytes.small_pool.freed', 1264594684416), ('allocated_bytes.small_pool.peak', 98147328), ('allocation.all.allocated', 13536346), ('allocation.all.current', 1498), ('allocation.all.freed', 13534848), ('allocation.all.peak', 1901), ('allocation.large_pool.allocated', 8861357), ('allocation.large_pool.current', 503), ('allocation.large_pool.freed', 8860854), ('allocation.large_pool.peak', 709), ('allocation.small_pool.allocated', 4674989), ('allocation.small_pool.current', 995), ('allocation.small_pool.freed', 4673994), ('allocation.small_pool.peak', 1223), ('inactive_split.all.allocated', 8131728), ('inactive_split.all.current', 198), ('inactive_split.all.freed', 8131530), ('inactive_split.all.peak', 259), ('inactive_split.large_pool.allocated', 5526211), ('inactive_split.large_pool.current', 115), ('inactive_split.large_pool.freed', 5526096), ('inactive_split.large_pool.peak', 158), ('inactive_split.small_pool.allocated', 2605517), ('inactive_split.small_pool.current', 83), ('inactive_split.small_pool.freed', 2605434), ('inactive_split.small_pool.peak', 139), ('inactive_split_bytes.all.allocated', 43285422379520), ('inactive_split_bytes.all.current', 349962240), ('inactive_split_bytes.all.freed', 43285072417280), ('inactive_split_bytes.all.peak', 1124942336), ('inactive_split_bytes.large_pool.allocated', 41993307377664), ('inactive_split_bytes.large_pool.current', 336941568), ('inactive_split_bytes.large_pool.freed', 41992970436096), ('inactive_split_bytes.large_pool.peak', 1111313408), ('inactive_split_bytes.small_pool.allocated', 1292115001856), ('inactive_split_bytes.small_pool.current', 13020672), ('inactive_split_bytes.small_pool.freed', 1292101981184), ('inactive_split_bytes.small_pool.peak', 41067520), ('max_split_size', -1), ('num_alloc_retries', 9), ('num_ooms', 3), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 57210690882966), ('requested_bytes.all.current', 15139358588), ('requested_bytes.all.freed', 57195551524378), ('requested_bytes.all.peak', 15422344572), ('requested_bytes.large_pool.allocated', 55946601959408), ('requested_bytes.large_pool.current', 15072738176), ('requested_bytes.large_pool.freed', 55931529221232), ('requested_bytes.large_pool.peak', 15355329408), ('requested_bytes.small_pool.allocated', 1264088923558), ('requested_bytes.small_pool.current', 66620412), ('requested_bytes.small_pool.freed', 1264022303146), ('requested_bytes.small_pool.peak', 98096568), ('reserved_bytes.all.allocated', 7465078882304), ('reserved_bytes.all.current', 15667822592), ('reserved_bytes.all.freed', 7449411059712), ('reserved_bytes.all.peak', 15667822592), ('reserved_bytes.large_pool.allocated', 7411744112640), ('reserved_bytes.large_pool.current', 15588130816), ('reserved_bytes.large_pool.freed', 7396155981824), ('reserved_bytes.large_pool.peak', 15588130816), ('reserved_bytes.small_pool.allocated', 53334769664), ('reserved_bytes.small_pool.current', 79691776), ('reserved_bytes.small_pool.freed', 53255077888), ('reserved_bytes.small_pool.peak', 111149056), ('segment.all.allocated', 266131), ('segment.all.current', 197), ('segment.all.freed', 265934), ('segment.all.peak', 250), ('segment.large_pool.allocated', 240699), ('segment.large_pool.current', 159), ('segment.large_pool.freed', 240540), ('segment.large_pool.peak', 207), ('segment.small_pool.allocated', 25432), ('segment.small_pool.current', 38), ('segment.small_pool.freed', 25394), ('segment.small_pool.peak', 53)])\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_stats())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atfskPxg5H-G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}